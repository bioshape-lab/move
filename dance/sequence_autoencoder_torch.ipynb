{"cells":[{"cell_type":"markdown","metadata":{"id":"0yrkKfy4pSpJ"},"source":["# Setup and load data:"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"GZuVnOoopkSZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645223734794,"user_tz":480,"elapsed":2069,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"}},"outputId":"f4cb4c55-c2ce-4944-8a92-0c75d3a27e43"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1645223640511,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"bcPWEgcwpu82","outputId":"48ce8b47-f366-4d22-d40c-7369287c315c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["#%cd /dance\n","\n","import os\n","import sys\n","import warnings\n","\n","print(os.getcwd())\n","\n","sys.path.append(os.path.dirname(os.getcwd()))\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":428,"status":"ok","timestamp":1645223642253,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"RllAvCYlpSpP","outputId":"c7911efa-e434-4e09-c3d8-e95e8218da9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1-DJonK3jse0NWdbyWWAIrK_srm2hV5ug/move/dance\n"]}],"source":["%cd /content/drive/MyDrive/colab-github/move/dance"]},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJFFGuTDmxoc","executionInfo":{"status":"ok","timestamp":1645223648055,"user_tz":480,"elapsed":4261,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"}},"outputId":"19316490-fccb-4eaf-a3f6-d0f79f3ff7d2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.6.3\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":11137,"status":"ok","timestamp":1645223666847,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"PDlwWRCmpSpP"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","import tensorflow as tf\n","from functions.draft_torch_lstm import *\n"]},{"cell_type":"code","source":["torch.cuda.is_available()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5IXAnhFBrRjm","executionInfo":{"status":"ok","timestamp":1645223675142,"user_tz":480,"elapsed":518,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"}},"outputId":"575634c4-cb7b-43ad-a010-fe19cd1ad42b"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u54tYqKplcYC","executionInfo":{"status":"ok","timestamp":1645223680021,"user_tz":480,"elapsed":1211,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"}},"outputId":"3354ed1d-67ba-46d8-b7d6-37d8634973fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1-DJonK3jse0NWdbyWWAIrK_srm2hV5ug/move/dance\n","loading: betternot_and_retrograde\n","\t Shape: (9925, 53, 3)\n","\t Min: [-1.96127391 -4.00212193 -0.15710293]\n","\t Max: [2.1591475  3.52660036 2.09531498]\n","loading: knownbetter\n","\t Shape: (6649, 53, 3)\n","\t Min: [-2.42928696 -3.50348401 -0.48134506]\n","\t Max: [2.13536358 3.36868167 2.0231936 ]\n","Full data shape: (16574, 53, 3)\n"]}],"source":["print(os.getcwd())\n","\n","ds_all, ds_all_centered, datasets, datasets_centered, ds_counts = load_data()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X08k7NzilcYD","executionInfo":{"status":"ok","timestamp":1645223687993,"user_tz":480,"elapsed":108,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"}},"outputId":"1c378413-d8be-4af9-c893-937a516da88e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":204,"status":"ok","timestamp":1645223703085,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"t7rANtV571oe","outputId":"88ccbf8b-0814-414f-d996-d3ffa1f6660d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(16574, 159)"]},"metadata":{},"execution_count":10}],"source":["ds_all.shape\n","my_data = ds_all.reshape((ds_all.shape[0], -1))\n","my_data.shape"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"b69hmt-2lcYF","executionInfo":{"status":"ok","timestamp":1645223709874,"user_tz":480,"elapsed":1902,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"}}},"outputs":[],"source":["#Make seq_data that will have shape \n","# [number of seq, 128, input_features]\n","seq_len=128\n","seq_data = np.zeros((my_data.shape[0]-seq_len, seq_len, my_data.shape[1]))\n","for i in range((ds_all.shape[0]-seq_len)):\n","    seq_data[i] = my_data[i:i+seq_len]\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q802WmaolcYF","executionInfo":{"status":"ok","timestamp":1645223712836,"user_tz":480,"elapsed":258,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"}},"outputId":"f4e40ea5-246a-4873-c62b-a1efbb7cdb9a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(16446, 128, 159)"]},"metadata":{},"execution_count":12}],"source":["seq_data.shape"]},{"cell_type":"markdown","metadata":{"id":"-f_uD8-WpSpR"},"source":["# Build the autoencoder for poses"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":336,"status":"ok","timestamp":1645223715880,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"F_Lv7HyeQ9yb"},"outputs":[],"source":["#Initialize encoder and decoder\n","my_encoder = LstmEncoder(input_features=3*53, h_features_loop=32, latent_dim=32)\n","my_decoder = LstmDecoder(n_layers=2, output_features=3*53, h_features_loop=32, latent_dim=32, seq_len=128)\n","my_lstmvae = LstmVAE()"]},{"cell_type":"markdown","metadata":{"id":"cvtzDvlpAfEN"},"source":["# Running the Autoencoder"]},{"cell_type":"markdown","metadata":{"id":"_1szXAU-AjAA"},"source":["Set the batch size and learning rate."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":151,"status":"ok","timestamp":1645225266806,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"hUBlP_HeAbIp"},"outputs":[],"source":["batchsize = 8\n","learning_rate= 3e-5"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":1078,"status":"ok","timestamp":1645225269816,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"cQvFN7LAN2gY"},"outputs":[],"source":["import torch\n","data_torch = torch.utils.data.DataLoader(seq_data, \n","    batch_size=batchsize, num_workers=2)\n","\n","x_input = torch.tensor(data_torch.dataset, dtype=torch.float32)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":308,"status":"ok","timestamp":1645225272945,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"rsfAn9uPQ9nx","outputId":"2559053d-f803-43de-f4f1-a09fbff97345"},"outputs":[{"output_type":"stream","name":"stdout","text":["starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n"]}],"source":["z_torch, mu, logvar = my_encoder(x_input[:8])\n","#add print statements to see where it crashes or infinitely loops\n","#setup_gpus\n","#do we need to update driver?"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2roP3vXlcYH","executionInfo":{"status":"ok","timestamp":1645225276994,"user_tz":480,"elapsed":131,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"}},"outputId":"9766239d-f5b7-4b0e-cead-70635219feb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 8, 32])\n"]}],"source":["print(z_torch.shape)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":121,"status":"ok","timestamp":1645225277940,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"DlGtp8R0TNJp","outputId":"4fc99cb3-4f01-484c-f0b0-c286d4663481"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([128, 8, 159])\n"]}],"source":["x_recon = my_decoder(z_torch)\n","print(x_recon.shape)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1645225279555,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"PCrx3QwZUvnq","outputId":"13adef16-14da-4628-8af5-287c1d1a56dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n"]}],"source":["x_mean, z_sample, z_mean, z_log_var = my_lstmvae(x_input[:8])"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xv9cAS1ylcYI","executionInfo":{"status":"ok","timestamp":1645225286535,"user_tz":480,"elapsed":5756,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"}},"outputId":"490c557b-5e9d-45ab-c449-956f523b929c"},"outputs":[{"output_type":"stream","name":"stdout","text":["(16446, 128, 159)\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([6, 128, 159])\n"]}],"source":["i = 0\n","\n","print(data_torch.dataset.shape)\n","for x in data_torch:\n","    print(x.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQEp8j39V6x0","outputId":"cc2b157a-4cf5-4998-9830-20fe1b830961"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n"]}],"source":["model = my_lstmvae\n","from torch.autograd import Variable\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n","loss_array=[]\n","\n","\n","for epoch in range(10):\n","    model.train()\n","    total_loss = 0\n","    for x in data_torch:\n","        x = Variable(x)\n","\n","        x_recon, z, z_mu, z_logvar = my_lstmvae(x.float())\n","        x_recon_batch_first=x_recon.reshape((x_recon.shape[1], x_recon.shape[0],x_recon.shape[2]))\n","        print(x_recon_batch_first.shape)\n","        loss = torch.mean(model.elbo(x, x_recon_batch_first, z, (z_mu, z_logvar)))\n","        loss.backward()\n","        loss_array.append(loss.item())\n","\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        total_loss += loss\n","\n","\n","loss_real_array = np.asarray(loss_array)\n","x_axis = np.arange(0, len(loss_real_array))\n","plt.figure(figsize=(12,8))\n","plt.plot(x_axis, loss_real_array)\n","plt.ylabel('Loss')\n","plt.xlabel('Batch')\n","plt.title('Batch size = {}, Learning rate = {}'.format(batchsize, learning_rate))\n","plt.savefig('Training_Loss_vs_Batch/batch_{}_lr_{}.png'.format(batchsize, learning_rate))      "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":530},"executionInfo":{"elapsed":599,"status":"ok","timestamp":1642810516869,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"WxVcFc3iIrHZ","outputId":"5e0e8a5f-a1fc-4438-a0fb-19c8ef385650"},"outputs":[{"data":{"text/plain":["<Figure size 1200x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","loss_real_array = np.asarray(loss_array)\n","x_axis = np.arange(0, len(loss_real_array))\n","plt.figure(figsize=(12,8))\n","plt.plot(x_axis, loss_real_array)\n","plt.ylabel('Loss')\n","plt.xlabel('Batch')\n","plt.title('Batch size = {}, Learning rate = {}'.format(batchsize, learning_rate))\n","plt.savefig('Training_Loss_vs_Batch/batch_{}_lr_{}.png'.format(batchsize, learning_rate))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"J4-I2yJsDyW6"},"source":["## Mariel's Autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"obLdZNvWpSpY"},"outputs":[],"source":["import numpy as np\n","from keras import backend as K\n","from keras.layers import Dense, Dropout, Flatten, Input, Lambda, LeakyReLU, Reshape\n","from keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","\n","class Autoencoder:\n","    def __init__(\n","        self,\n","        n_verts=0,\n","        n_dims=3,\n","        latent_dim=2,\n","        n_layers=2,\n","        n_units=128,\n","        relu=False,\n","        add_random_offsets=False,\n","        dropout=False,\n","    ):\n","        if not n_verts:\n","            raise Exception(\"Please provide the number of vertices `n_verts`\")\n","        self.n_verts = n_verts  # input vert count\n","        self.n_dims = n_dims  # input dimensions\n","        self.relu = relu\n","        # whether to add relu layers in encoder/decoder\n","        self.dropout = dropout  # whether to add dropout layers in encoder/decoder\n","        self.latent_dim = latent_dim\n","        self.n_layers = n_layers\n","        self.n_units = n_units\n","        self.encoder = self.build_encoder()\n","        self.decoder = self.build_decoder()\n","        # attach the encoder and decoder\n","        i = Input((self.n_verts, self.n_dims))\n","        if add_random_offsets:\n","            random_offsets = (\n","                K.cast(K.learning_phase(), \"float\")\n","                * K.random_uniform((K.shape(i)[0], 1, 3))\n","                * K.constant([[[1, 1, 0]]])\n","            )\n","            offset_layer = Lambda(lambda x: x + random_offsets)\n","            offset_layer.uses_learning_phase = True\n","            i_offset = offset_layer(i)\n","        else:\n","            i_offset = i\n","        z = self.encoder(i_offset)  # push observations into latent space\n","        o = self.decoder(z)  # project from latent space to feature space\n","        if add_random_offsets:\n","            o = Lambda(lambda x: x - random_offsets)(o)\n","        self.model = Model(inputs=[i], outputs=[o])\n","        self.model.compile(loss=\"mse\", optimizer=Adam(lr=1e-4))\n","\n","    def build_encoder(self):\n","        i = Input((self.n_verts, self.n_dims))\n","        h = i\n","        h = Flatten()(h)\n","        for _ in range(self.n_layers):\n","            h = Dense(self.n_units)(h)\n","            if self.relu:\n","                h = LeakyReLU(alpha=0.2)(h)\n","            if self.dropout:\n","                h = Dropout(0.4)(h)\n","        o = Dense(self.latent_dim)(h)\n","        return Model(inputs=[i], outputs=[o])\n","\n","    def build_decoder(self):\n","        i = Input((self.latent_dim,))\n","        h = i\n","        for _ in range(self.n_layers):\n","            h = Dense(self.n_units)(h)\n","            if self.relu:\n","                h = LeakyReLU(alpha=0.2)(h)\n","            if self.dropout:\n","                h = Dropout(0.4)(h)\n","        h = Dense(self.n_verts * self.n_dims)(h)\n","        o = Reshape((self.n_verts, self.n_dims))(h)  # predict 1 frame\n","        return Model(inputs=[i], outputs=[o])\n","\n","    def train(self, X, n_epochs=10000):\n","        for idx in range(n_epochs):\n","            i = np.random.randint(0, X.shape[1] - 1)  # sample idx\n","            frame = np.expand_dims(\n","                X[:, i : i + 1, :].squeeze(), axis=0\n","            )  # shape = 1 sample, v verts, d dims\n","            loss = self.model.train_on_batch(frame, frame)\n","            if idx == 0:\n","                print(frame.shape)\n","            if idx % 1000 == 0:\n","                print(\" * training idx\", idx, \"loss\", loss)\n","\n","    def get_predictions(self, X, n_frames=50, start_frame=0):\n","        \"\"\"Return the model's predictions of observations from X in shape of X\"\"\"\n","        predictions = []\n","        for i in range(start_frame, start_frame + n_frames, 1):\n","            x = np.expand_dims(X[:, i : i + 1, :].squeeze(), axis=0)\n","            predictions.append(self.model.predict(x))\n","        return np.swapaxes(np.vstack(predictions), 0, 1)\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"sequence_autoencoder_torch.ipynb","provenance":[]},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"choreo","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}