{"cells":[{"cell_type":"markdown","metadata":{"id":"0yrkKfy4pSpJ"},"source":["# Setup and load data:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"GZuVnOoopkSZ"},"outputs":[],"source":["SERVER = \"pod\" # \"colab\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":546,"status":"ok","timestamp":1642796537169,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"bcPWEgcwpu82","outputId":"8daa45d2-00e7-4bcc-d1ba-f3674b1f6c7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/papillon/move/dance\n"]}],"source":["#%cd /dance\n","\n","import os\n","import sys\n","import warnings\n","\n","print(os.getcwd())\n","\n","sys.path.append(os.path.dirname(os.getcwd()))\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":181,"status":"ok","timestamp":1642796542271,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"RllAvCYlpSpP","outputId":"1514f70f-316c-40bf-91da-1e92f24b3d98"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/colab-github/move/dance'\n","/home/papillon/move/dance\n"]}],"source":["%cd /content/drive/MyDrive/colab-github/move/dance"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6186,"status":"ok","timestamp":1642796550511,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"PDlwWRCmpSpP","outputId":"a5882945-d3a2-43c7-b664-0c73d59b7e1d"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","import tensorflow as tf\n","from functions.draft_torch_lstm import *\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["()"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["setup_gpus()\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/papillon/move/dance\n","loading: betternot_and_retrograde\n","\t Shape: (9925, 53, 3)\n","\t Min: [-1.96127391 -4.00212193 -0.15710293]\n","\t Max: [2.1591475  3.52660036 2.09531498]\n","loading: knownbetter\n","\t Shape: (6649, 53, 3)\n","\t Min: [-2.42928696 -3.50348401 -0.48134506]\n","\t Max: [2.13536358 3.36868167 2.0231936 ]\n","Full data shape: (16574, 53, 3)\n"]}],"source":["print(os.getcwd())\n","\n","ds_all, ds_all_centered, datasets, datasets_centered, ds_counts = load_data()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["False"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":192,"status":"ok","timestamp":1642796551897,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"t7rANtV571oe","outputId":"396f2910-7ea6-4f65-aeea-9bb36db34e94"},"outputs":[{"data":{"text/plain":["(16574, 159)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["ds_all.shape\n","my_data = ds_all.reshape((ds_all.shape[0], -1))\n","my_data.shape"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["#Make seq_data that will have shape \n","# [number of seq, 128, input_features]\n","seq_len=128\n","seq_data = np.zeros((my_data.shape[0]-seq_len, seq_len, my_data.shape[1]))\n","for i in range((ds_all.shape[0]-seq_len)):\n","    seq_data[i] = my_data[i:i+seq_len]\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["(16446, 128, 159)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["seq_data.shape"]},{"cell_type":"markdown","metadata":{"id":"-f_uD8-WpSpR"},"source":["# Build the autoencoder for poses"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":203,"status":"ok","timestamp":1642804977630,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"F_Lv7HyeQ9yb","outputId":"83dc25bd-9c4d-4f46-e10b-b22d8527fe7e"},"outputs":[],"source":["#Initialize encoder and decoder\n","my_encoder = LstmEncoder(input_features=3*53, h_features_loop=32, latent_dim=32)\n","my_decoder = LstmDecoder(n_layers=2, output_features=3*53, h_features_loop=32, latent_dim=32, seq_len=128)\n","my_lstmvae = LstmVAE()"]},{"cell_type":"markdown","metadata":{"id":"cvtzDvlpAfEN"},"source":["# Running the Autoencoder"]},{"cell_type":"markdown","metadata":{"id":"_1szXAU-AjAA"},"source":["Set the batch size and learning rate."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":217,"status":"ok","timestamp":1642811167157,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"hUBlP_HeAbIp"},"outputs":[],"source":["batchsize = 8\n","learning_rate= 3e-5"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":223,"status":"ok","timestamp":1642811168209,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"cQvFN7LAN2gY"},"outputs":[],"source":["import torch\n","data_torch = torch.utils.data.DataLoader(seq_data, \n","    batch_size=batchsize, num_workers=2)\n","\n","x_input = torch.tensor(data_torch.dataset, dtype=torch.float32)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":198,"status":"ok","timestamp":1642811169059,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"rsfAn9uPQ9nx","outputId":"b44ce76a-f880-46be-c662-be370fb72fa8"},"outputs":[{"name":"stdout","output_type":"stream","text":["starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n"]}],"source":["z_torch, mu, logvar = my_encoder(x_input[:8])\n","#add print statements to see where it crashes or infinitely loops\n","#setup_gpus\n","#do we need to update driver?"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 8, 32])\n"]}],"source":["print(z_torch.shape)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":183,"status":"ok","timestamp":1642811170557,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"DlGtp8R0TNJp","outputId":"89153c97-ac65-46a3-b970-a0113761a847"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([128, 8, 159])\n"]}],"source":["x_recon = my_decoder(z_torch)\n","print(x_recon.shape)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5979,"status":"ok","timestamp":1642811177507,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"PCrx3QwZUvnq","outputId":"afe88ca8-7e11-4c12-d1fd-de75364abe93"},"outputs":[{"name":"stdout","output_type":"stream","text":["starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n"]}],"source":["x_mean, z_sample, z_mean, z_log_var = my_lstmvae(x_input[:8])"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(16446, 128, 159)\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([8, 128, 159])\n","torch.Size([6, 128, 159])\n"]}],"source":["i = 0\n","\n","print(data_torch.dataset.shape)\n","for x in data_torch:\n","    print(x.shape)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":513},"executionInfo":{"elapsed":117430,"status":"ok","timestamp":1642811295945,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"ZQEp8j39V6x0","outputId":"1481c29f-9504-49b1-971d-bbe796d9b84b"},"outputs":[{"name":"stdout","output_type":"stream","text":["starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n","starting the forward of encoder. the first step is calling layer lstm1\n","done layer lstm1. It returned h1 of shape torch.Size([8, 128, 32]) and h1_T of shapetorch.Size([1, 8, 32])\n","Now starting the loop of 2-1 lstm layers\n","this is loop iteration 0. Calling layer lstm2\n","done layer lstm 2. lstm2 returns h2 of shape torch.Size([8, 128, 32]) and h2_T of shape torch.Size([1, 8, 32])\n","Now computing the encoder output.\n","calling mean_block\n","z_mean has shape torch.Size([1, 8, 32])\n","z_logvar has shape torch.Size([1, 8, 32])\n","reparametrize function called\n","made std\n","print made eps\n","z_sample has shape torch.Size([1, 8, 32])\n","encoder is done\n","torch.Size([8, 128, 159])\n","x_in has shape torch.Size([8, 128, 159]) and x_out has shape torch.Size([8, 128, 159])\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-162706624bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/choreo/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/choreo/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/choreo/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/choreo/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model = my_lstmvae\n","from torch.autograd import Variable\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n","loss_array=[]\n","\n","\n","for epoch in range(10):\n","    model.train()\n","    total_loss = 0\n","    for x in data_torch:\n","        x = Variable(x)\n","\n","        x_recon, z, z_mu, z_logvar = my_lstmvae(x.float())\n","        x_recon_batch_first=x_recon.reshape((x_recon.shape[1], x_recon.shape[0],x_recon.shape[2]))\n","        print(x_recon_batch_first.shape)\n","        loss = torch.mean(model.elbo(x, x_recon_batch_first, z, (z_mu, z_logvar)))\n","        loss.backward()\n","        loss_array.append(loss.item())\n","\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        total_loss += loss\n","\n","\n","loss_real_array = np.asarray(loss_array)\n","x_axis = np.arange(0, len(loss_real_array))\n","plt.figure(figsize=(12,8))\n","plt.plot(x_axis, loss_real_array)\n","plt.ylabel('Loss')\n","plt.xlabel('Batch')\n","plt.title('Batch size = {}, Learning rate = {}'.format(batchsize, learning_rate))\n","plt.savefig('Training_Loss_vs_Batch/batch_{}_lr_{}.png'.format(batchsize, learning_rate))      "]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":530},"executionInfo":{"elapsed":599,"status":"ok","timestamp":1642810516869,"user":{"displayName":"Mathilde Papillon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02648754252281357535"},"user_tz":480},"id":"WxVcFc3iIrHZ","outputId":"5e0e8a5f-a1fc-4438-a0fb-19c8ef385650"},"outputs":[{"data":{"text/plain":["<Figure size 1200x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","loss_real_array = np.asarray(loss_array)\n","x_axis = np.arange(0, len(loss_real_array))\n","plt.figure(figsize=(12,8))\n","plt.plot(x_axis, loss_real_array)\n","plt.ylabel('Loss')\n","plt.xlabel('Batch')\n","plt.title('Batch size = {}, Learning rate = {}'.format(batchsize, learning_rate))\n","plt.savefig('Training_Loss_vs_Batch/batch_{}_lr_{}.png'.format(batchsize, learning_rate))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"J4-I2yJsDyW6"},"source":["## Mariel's Autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"obLdZNvWpSpY"},"outputs":[],"source":["import numpy as np\n","from keras import backend as K\n","from keras.layers import Dense, Dropout, Flatten, Input, Lambda, LeakyReLU, Reshape\n","from keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","\n","class Autoencoder:\n","    def __init__(\n","        self,\n","        n_verts=0,\n","        n_dims=3,\n","        latent_dim=2,\n","        n_layers=2,\n","        n_units=128,\n","        relu=False,\n","        add_random_offsets=False,\n","        dropout=False,\n","    ):\n","        if not n_verts:\n","            raise Exception(\"Please provide the number of vertices `n_verts`\")\n","        self.n_verts = n_verts  # input vert count\n","        self.n_dims = n_dims  # input dimensions\n","        self.relu = relu\n","        # whether to add relu layers in encoder/decoder\n","        self.dropout = dropout  # whether to add dropout layers in encoder/decoder\n","        self.latent_dim = latent_dim\n","        self.n_layers = n_layers\n","        self.n_units = n_units\n","        self.encoder = self.build_encoder()\n","        self.decoder = self.build_decoder()\n","        # attach the encoder and decoder\n","        i = Input((self.n_verts, self.n_dims))\n","        if add_random_offsets:\n","            random_offsets = (\n","                K.cast(K.learning_phase(), \"float\")\n","                * K.random_uniform((K.shape(i)[0], 1, 3))\n","                * K.constant([[[1, 1, 0]]])\n","            )\n","            offset_layer = Lambda(lambda x: x + random_offsets)\n","            offset_layer.uses_learning_phase = True\n","            i_offset = offset_layer(i)\n","        else:\n","            i_offset = i\n","        z = self.encoder(i_offset)  # push observations into latent space\n","        o = self.decoder(z)  # project from latent space to feature space\n","        if add_random_offsets:\n","            o = Lambda(lambda x: x - random_offsets)(o)\n","        self.model = Model(inputs=[i], outputs=[o])\n","        self.model.compile(loss=\"mse\", optimizer=Adam(lr=1e-4))\n","\n","    def build_encoder(self):\n","        i = Input((self.n_verts, self.n_dims))\n","        h = i\n","        h = Flatten()(h)\n","        for _ in range(self.n_layers):\n","            h = Dense(self.n_units)(h)\n","            if self.relu:\n","                h = LeakyReLU(alpha=0.2)(h)\n","            if self.dropout:\n","                h = Dropout(0.4)(h)\n","        o = Dense(self.latent_dim)(h)\n","        return Model(inputs=[i], outputs=[o])\n","\n","    def build_decoder(self):\n","        i = Input((self.latent_dim,))\n","        h = i\n","        for _ in range(self.n_layers):\n","            h = Dense(self.n_units)(h)\n","            if self.relu:\n","                h = LeakyReLU(alpha=0.2)(h)\n","            if self.dropout:\n","                h = Dropout(0.4)(h)\n","        h = Dense(self.n_verts * self.n_dims)(h)\n","        o = Reshape((self.n_verts, self.n_dims))(h)  # predict 1 frame\n","        return Model(inputs=[i], outputs=[o])\n","\n","    def train(self, X, n_epochs=10000):\n","        for idx in range(n_epochs):\n","            i = np.random.randint(0, X.shape[1] - 1)  # sample idx\n","            frame = np.expand_dims(\n","                X[:, i : i + 1, :].squeeze(), axis=0\n","            )  # shape = 1 sample, v verts, d dims\n","            loss = self.model.train_on_batch(frame, frame)\n","            if idx == 0:\n","                print(frame.shape)\n","            if idx % 1000 == 0:\n","                print(\" * training idx\", idx, \"loss\", loss)\n","\n","    def get_predictions(self, X, n_frames=50, start_frame=0):\n","        \"\"\"Return the model's predictions of observations from X in shape of X\"\"\"\n","        predictions = []\n","        for i in range(start_frame, start_frame + n_frames, 1):\n","            x = np.expand_dims(X[:, i : i + 1, :].squeeze(), axis=0)\n","            predictions.append(self.model.predict(x))\n","        return np.swapaxes(np.vstack(predictions), 0, 1)\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"pose_autoencoder_torch.ipynb","provenance":[]},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"choreo","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}
